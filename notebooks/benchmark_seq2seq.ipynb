{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmarking Seq2Seq on Chaotic Oscillators\n",
    "\n",
    "Seq2Seq models have been shown to be effective in modeling chaotic systems. This benchmark contains code to train and evaluate Seq2Seq models on the following chaotic systems :\n",
    "  - [Lorenz](###Lorenz)\n",
    "  - [Fibonacci](###Fibonacci)\n",
    "  - [Van Der Pol](###Van-Der-Pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "try:\n",
    "    from src.utils.seed_utils import set_global_seeds\n",
    "except ImportError:\n",
    "    raise ImportError(\"Cannot import module. Make sure that the project is on the path\")\n",
    "\n",
    "SEED = 42\n",
    "set_global_seeds(seed=SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "INPUT_LENGTH = 50\n",
    "TARGET_LENGTH = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lorenz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_sources.lorenz import LorenzOscillator\n",
    "from src.utils.dataset_utils import DatasetUtils\n",
    "\n",
    "# Example parameters\n",
    "LORENZ_INPUT_LAYERS = 3\n",
    "LORENZ_OUTPUT_LAYERS = 3\n",
    "\n",
    "# Lorenz oscillator parameters\n",
    "initial_state = [1.0, 1.0, 1.0]\n",
    "t_span = (0, 100)\n",
    "max_step = 1e-2\n",
    "\n",
    "lorenz = LorenzOscillator(sigma=10, rho=28, beta=8.0 / 3.0)\n",
    "\n",
    "lorenz_dataset = lorenz.preprocess_and_create_dataset(\n",
    "    initial_state=initial_state,\n",
    "    t_span=t_span,\n",
    "    max_step=max_step,\n",
    "    input_length=INPUT_LENGTH,\n",
    "    target_length=TARGET_LENGTH,\n",
    ")\n",
    "\n",
    "# Split into train/test loaders\n",
    "lorenz_train_loader, lorenz_test_loader = DatasetUtils().train_test_split(\n",
    "    dataset=lorenz_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    train_ratio=0.8,\n",
    "    shuffle_train=False,\n",
    "    shuffle_test=False,\n",
    ")\n",
    "\n",
    "# Visualize the 3D Lorenz trajectory\n",
    "lorenz.plot_trajectory(initial_state=initial_state, t_span=t_span, max_step=max_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dataset_utils import DatasetUtils\n",
    "from src.data_sources.emg import EMGRawDataset\n",
    "\n",
    "root_dir = \"../data/silent_speech_dataset/raw/nonparallel_data\"\n",
    "\n",
    "emg_dataset = EMGRawDataset(root_dir)\n",
    "\n",
    "preprocessed_data = emg_dataset.load_and_preprocess_data(\n",
    "    max_files=10,\n",
    ")\n",
    "\n",
    "timeseries_dataset = emg_dataset.create_dataset(\n",
    "    input_length=INPUT_LENGTH, target_length=TARGET_LENGTH\n",
    ")\n",
    "\n",
    "emg_raw_train_loader, emg_raw_test_loader = DatasetUtils().train_test_split(\n",
    "    dataset=timeseries_dataset,\n",
    "    batch_size=32,\n",
    "    train_ratio=0.8,\n",
    "    shuffle_train=False,\n",
    "    shuffle_test=False,\n",
    ")\n",
    "\n",
    "num_channels = preprocessed_data.shape[1]\n",
    "print(f\"Number of EMG channels: {num_channels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from src.schemas.paths_schema import PathsSchema\n",
    "from src.utils.torch_utils import criterion_choice\n",
    "from src.utils.tensorboard_utils import launch_tensorboard\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def train_seq2seq(\n",
    "    model: torch.nn.Module,\n",
    "    train_data: Any,\n",
    "    test_data: Any,\n",
    "    device: torch.device,\n",
    "    SEED: int,\n",
    "    BATCH_SIZE: int,\n",
    "    teacher_forcing_ratio: float = 0.5,\n",
    "    loss_function: str = \"mse\",\n",
    "    learning_rate: float = 1e-6,\n",
    "    num_epochs: int = 1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains a provided Seq2Seq model using the given training and testing data loaders.\n",
    "    Logs training and test losses to TensorBoard and saves the final model.\n",
    "    \n",
    "    Args:\n",
    "        model: The Seq2Seq model instance. It must accept inputs (x, y, teacher_forcing_ratio).\n",
    "        train_data: DataLoader for training data (expects batches of (x, y)).\n",
    "        test_data: DataLoader for testing data (expects batches of (x, y)).\n",
    "        device: Device to use for training (e.g., torch.device(\"cuda\") or torch.device(\"cpu\")).\n",
    "        SEED: Seed used for experiment naming.\n",
    "        BATCH_SIZE: Batch size used during training.\n",
    "        teacher_forcing_ratio: Probability of using teacher forcing during training.\n",
    "        loss_function: Name of the loss function (e.g., \"mse\").\n",
    "        learning_rate: Learning rate for the optimizer.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        \n",
    "    Returns:\n",
    "        model: The trained Seq2Seq model.\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    criterion = criterion_choice(loss_function)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Setup paths for TensorBoard logs and model saving.\n",
    "    model_name = os.path.join(\n",
    "        \"lorenz\",\n",
    "        \"seq2seq\",\n",
    "        f\"seed_{SEED}_batch_size_{BATCH_SIZE}_num_epochs_{num_epochs}_lr_{learning_rate}\"\n",
    "    )\n",
    "    paths = PathsSchema(model_name=model_name)\n",
    "    writer = SummaryWriter(log_dir=paths.tensorboard_log_model)\n",
    "    \n",
    "    # Launch TensorBoard (optional)\n",
    "    launch_tensorboard(paths.tensorboard_log_model)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Training Seq2Seq\"):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for x, y in train_data:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, y, teacher_forcing_ratio=teacher_forcing_ratio)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        avg_train_loss = train_loss / len(train_data)\n",
    "        \n",
    "        # Evaluation phase\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in test_data:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                outputs = model(x, y, teacher_forcing_ratio=0)\n",
    "                loss = criterion(outputs, y.squeeze(dim=1))\n",
    "                test_loss += loss.item()\n",
    "        avg_test_loss = test_loss / len(test_data)\n",
    "        \n",
    "        writer.add_scalars(\"Loss\", {\"train\": avg_train_loss, \"test\": avg_test_loss}, epoch)\n",
    "        \n",
    "        if (epoch + 1) % max(1, (num_epochs // 10)) == 0:\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}\")\n",
    "    \n",
    "    writer.close()\n",
    "    \n",
    "    # Save the final model.\n",
    "    model_filename = (\n",
    "        paths.model_path\n",
    "        + f\"epoch_{num_epochs}_train_loss_{avg_train_loss:.4f}_test_loss_{avg_test_loss:.4f}.pt\"\n",
    "    )\n",
    "    torch.save(model, model_filename)\n",
    "    print(f\"Model saved: '{model_filename}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lorenz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import seq2seq\n",
    "from src.utils.plot_utils import plot_multistep_evaluation\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 128\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "teacher_forcing_ratio = 0.5\n",
    "loss_function = \"mse\"\n",
    "learning_rate = 1e-6\n",
    "num_epochs = 1000\n",
    "\n",
    "# Model setup\n",
    "encoder = seq2seq.Encoder(LORENZ_INPUT_LAYERS, hidden_size, num_layers, dropout).to(\n",
    "    device\n",
    ")\n",
    "decoder = seq2seq.Decoder(LORENZ_OUTPUT_LAYERS, hidden_size, num_layers, dropout).to(\n",
    "    device\n",
    ")\n",
    "model = seq2seq.Seq2Seq(encoder, decoder).to(device)\n",
    "trained_model = train_seq2seq(\n",
    "    model=model,\n",
    "    train_data=lorenz_train_loader,\n",
    "    test_data=lorenz_test_loader,\n",
    "    device=device,\n",
    "    SEED=SEED,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "    loss_function=loss_function,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "plot_multistep_evaluation(model, data=lorenz_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import seq2seq\n",
    "from src.utils.plot_utils import plot_multistep_evaluation\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_size = 32\n",
    "num_layers = 3\n",
    "dropout = 0.2\n",
    "teacher_forcing_ratio = 0.5\n",
    "loss_function = \"mse\"\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 10\n",
    "\n",
    "# Model setup\n",
    "encoder = seq2seq.Encoder(num_channels, hidden_size, num_layers, dropout).to(\n",
    "    device\n",
    ")\n",
    "decoder = seq2seq.Decoder(num_channels, hidden_size, num_layers, dropout).to(\n",
    "    device\n",
    ")\n",
    "model = seq2seq.Seq2Seq(encoder, decoder).to(device)\n",
    "trained_model = train_seq2seq(\n",
    "    model=model,\n",
    "    train_data=emg_raw_train_loader,\n",
    "    test_data=emg_raw_test_loader,\n",
    "    device=device,\n",
    "    SEED=SEED,\n",
    "    BATCH_SIZE=BATCH_SIZE,\n",
    "    teacher_forcing_ratio=teacher_forcing_ratio,\n",
    "    loss_function=loss_function,\n",
    "    learning_rate=learning_rate,\n",
    "    num_epochs=num_epochs,\n",
    ")\n",
    "\n",
    "plot_multistep_evaluation(model, data=emg_raw_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multistep_evaluation(model, data=emg_raw_test_loader, sample_idx=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IAR_SSR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
